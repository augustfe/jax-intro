{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Stage\n",
    "\n",
    "JAX is a numerical computing library that is designed to be composable, modular, and extensible. It is built on top of XLA, which is a compiler that can take a function and compile it into a highly optimized version that can run on a variety of hardware. JAX is designed to be a drop-in replacement for NumPy, but it also has a number of features that make it more powerful and flexible than NumPy.\n",
    "\n",
    "JAX is built from three main components:\n",
    "- `jax.vmap`: A function that takes a Python function and vectorizes it.\n",
    "- `jax.jit`: A function that creates a compiled and optimized version of a function.\n",
    "- `jax.grad`: A function that automatically differentiates another Python function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Installation\n",
    "\n",
    "Installing JAX is typically as simple as running:\n",
    "\n",
    "```bash\n",
    "$ pip install jax\n",
    "```\n",
    "\n",
    "For a more detailed installation guide, see the [JAX documentation](https://jax.readthedocs.io/en/latest/installation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using JAX as a Drop-In Replacement for NumPy\n",
    "\n",
    "JAX includes most of the functionality you might be familiar with from NumPy, rewritten to utilize XLA instructions and to be compatible with JAX's other features. This is provided through the `jax.numpy` module, typically aliased as `jnp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX can then be utilized in much the same way as NumPy. Taking the simple example of a matrix-vector product of\n",
    "\n",
    "```{math}\n",
    ":label: matvec\n",
    "y = Ax\n",
    "```\n",
    "\n",
    "for instance,\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 \\\\\n",
    "        2 & 3 \\\\\n",
    "        4 & 5\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        1 \\\\ 1\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        1 \\\\ 5 \\\\ 9\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "In NumPy, this could be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 5. 9.]\n"
     ]
    }
   ],
   "source": [
    "A = jnp.arange(6).reshape(3, 2)\n",
    "x = jnp.ones(2)\n",
    "\n",
    "print(A @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of JAX then comes from the concept of _composing_ functions, allowing complex operations to be natively written in a fashion closely resembeling their mathematical counterparts. Continuing with the matrix-vector product example, consider the case where instead of a single vector $x \\in \\mathbb{R}^d$, we have a set of $N$ vectors $X$. We are then interested in computing\n",
    "\n",
    "$$\n",
    "    y_i = Ax_{i} \\quad \\text{for } i  = 1, 2, \\ldots, N,\n",
    "$$\n",
    "\n",
    "for a matrix $A \\in \\mathbb{R}^{m \\times d}$. In NumPy, we would typically store these vectors in a matrix $X \\in \\mathbb{R}^{N \\times d}$ and compute the matrix-vector product $AX^T$. We would then be left with a matrix $Y \\in \\mathbb{R}^{m \\times N}$, where each column $Y_i$ corresponds to the result of the matrix-vector product $Ax_i$. Keeping track of these dimensionalities can quickly become cumbersome, especially when performed in sequence. A band-aid to this would be to actually compute $(AX^T)^T = XA^T$, however we are then further removed from the mathematical notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  10  18]\n",
      " [  2   0  -2]\n",
      " [  0   4   8]\n",
      " [ -3 -13 -23]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(2002)\n",
    "A = np.arange(6).reshape(3, 2)\n",
    "X = rng.integers(-3, 3, size=(4, 2))\n",
    "\n",
    "\n",
    "def apply_matrix(A: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    return X @ A.T\n",
    "\n",
    "\n",
    "Y = apply_matrix(A, X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative, seemingly little known, approach in NumPy would be to use `np.einsum`, which allows for Einstein summation notation. This is a powerful tool, but can be difficult to read and write, especially for more complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def apply_matrix_einsum(A: np.ndarray, X: np.ndarray) -> np.ndarray:\n",
    "    return np.einsum(\"ij,Nj->Ni\", A, X)\n",
    "\n",
    "\n",
    "Y_einsum = apply_matrix_einsum(A, X)\n",
    "print(np.allclose(Y, Y_einsum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In JAX, we can instead use `jax.vmap` to vectorize the matrix-vector product, allowing us to compute the matrix-vector product for each vector in $X$ in a single function call. This is done by simply wrapping the matrix-vector product in a call to `jax.vmap`. The result is a function that computes the matrix-vector product for each vector in $X$ and returns the result as a single array. This is done in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from jax import vmap, Array\n",
    "\n",
    "\n",
    "def apply_matrix_jax(A: Array, x: Array) -> Array:\n",
    "    return A @ x\n",
    "\n",
    "\n",
    "# Convert the NumPy arrays to JAX arrays\n",
    "A_jax = jnp.array(A)\n",
    "X_jax = jnp.array(X)\n",
    "\n",
    "vectorized_apply_matrix = vmap(apply_matrix_jax, in_axes=(None, 0))\n",
    "Y_vmap = vectorized_apply_matrix(A_jax, X_jax)\n",
    "\n",
    "print(np.allclose(Y, Y_vmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we simply define our function as in the same way as Equation {eq}`matvec`, and then _vectorize_ it through a call to `jax.vmap`. The `in_axes` argument specifies which axes of the input should be mapped over. By specifying `in_axes=(None, 0)`, we are telling `jax.vmap` to map over the over the first axis of the second argument (the vectors in $X$), while leaving the first argument (the matrix $A$) unchanged. The result is a function that computes the matrix-vector product for each vector in $X$ and returns the result as a single array. \n",
    "\n",
    "Note also the simple interplay between JAX and NumPy, where we can use NumPy to generate the matrix $A$ and the vectors $X$, and then use JAX to compute the matrix-vector product. We can then go back again to NumPy in order to verify the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
